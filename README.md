# Fairness-Aware Machine Learning using Counterfactual Explanations (COMPAS Dataset)

This repository contains the implementation of a fairness-focused machine learning study using the **COMPAS recidivism dataset**. The project identifies, evaluates, and mitigates algorithmic bias using a combination of **Counterfactual Explanations** (DiCE) and the **Fairlearn** fairness toolkit. The goal is to design a more transparent, responsible, and ethically-aware AI model that makes fairer decisions.

---

## Overview

Machine learning models often inherit bias present in the datasets used to train them. The COMPAS dataset is known to contain bias based on **race** and **sex**, reflecting historical inequalities. This project demonstrates:

- How biased models behave unfairly
- How counterfactual explanations reveal these biases
- How fairness constraints (Demographic Parity) help mitigate discrimination
- The trade-off between **fairness** and **accuracy**

---

## Objectives

- Detect discriminatory patterns in a baseline (biased) classifier  
- Generate **counterfactual explanations** by altering protected attributes  
- Evaluate fairness using **Fairlearn’s MetricFrame**  
- Mitigate bias using **Demographic Parity constraint**  
- Compare biased vs. mitigated model performance  
- Understand trade-offs between fairness and accuracy  

---

## Methodology

### **1. Baseline Model (Biased Model)**
A `RandomForestClassifier` is trained normally on the COMPAS dataset.  
This model inherits dataset bias and thus treats groups unfairly.

### **2. Counterfactual Explanations**
Using **DiCE**, counterfactual instances are generated by modifying:
- `race`
- `sex`

while keeping all other features constant.

Large shifts in predictions reveal **sensitive-attribute-driven bias**.

### **3. Fairness Evaluation**
Using **Fairlearn MetricFrame**, two fairness metrics are computed:
- **Accuracy per group**
- **Selection Rate per group**

This quantifies differences in model behavior for different racial/sex groups.

### **4. Bias Mitigation**
A new model is trained using:

✔ **Fairlearn’s ExponentiatedGradient**  
✔ **Demographic Parity constraint**

This constraint ensures **equal opportunity for all groups** to receive a favorable prediction.

### **5. Re-evaluation & Comparison**
After mitigation:
- Prediction disparity decreases
- Counterfactual flips reduce
- Some accuracy trade-off is observed

---

## Key Results

- The biased model showed significant changes in prediction when protected attributes were altered.
- Fairlearn metrics revealed unequal selection rates across race/sex groups.
- The fairness-constrained model reduced these disparities, though at the cost of reduced accuracy.
- Counterfactual behavior became more stable after mitigation but not perfectly bias-free.
- The results have been attached in the visuals folder.

---

## Conclusion

This project demonstrates how machine learning models trained on biased data can produce discriminatory outcomes. By integrating **counterfactual explanations** with **fairness constraints**, we are able to detect, understand, and partially mitigate this bias. The study highlights the inherent **trade-off between fairness and accuracy**, and ultimately advocates for more **transparent and ethically-aware AI systems**.

---

## Requirements

- Python 3.9+
- scikit-learn
- pandas
- numpy
- matplotlib
- fairlearn
- dice-ml
- seaborn 

---

## Note for Users

First run the scripts: "InitialModelTraining.py" and "fairnessModelTraining.py".
This might take some time as it would create our model.
The rest of the scripts are to test the models' fairness and produce counterfactuals.